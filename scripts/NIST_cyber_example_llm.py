# -*- coding: utf-8 -*-
"""r_cyber_quickstart_llm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18124U7pKYR2vOcr2MFZG7-pd5clbZeHL

# 📚  LLM Quickstart - Applied to NIST AI Risk Management Framework

Giskard is an open-source framework for testing all ML models, from LLMs to tabular models. Don't hesitate to give the project a [star on GitHub](https://github.com/Giskard-AI/giskard) ⭐️ if you find it useful!

In this tutorial we will use Giskard's LLM Scan to automatically detect issues on a Retrieval Augmented Generation (RAG) task. We will test a model that answers questions about climate change, based on the [2024 AI Risk Management Framework](https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf) by the IPCC.

Our platform supports a variety of LLMs to run the scan, including but not limited to OpenAI GPT models, Azure OpenAI, Ollama, and Mistral. For the purpose of this example we will use the OpenAI Client but to configure a different language model follow our detailed instructions on the [🤖 Setting up the LLM Client page](../../open_source/setting_up/index.md) to set up your chosen LLM client.

Use-case:  

* QA over the NIST AI Risk Management Framework report
* Foundational model: *gpt-3.5-turbo-instruct*
* Context: [AI Risk Management Framework](https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf)

## Install dependencies

Make sure to install the `giskard[llm]` flavor of Giskard, which includes support for LLM models.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install "giskard[llm]" --upgrade

""" We also install the project-specific dependencies for this tutorial."""

# Commented out IPython magic to ensure Python compatibility.
# %pip install langchain langchain-openai langchain-community pypdf faiss-cpu openai tiktoken

"""## Setup OpenAI

LLM scan requires an OpenAI API key. We set it here:
"""

import os

# Set the OpenAI API Key environment variable.
os.environ["OPENAI_API_KEY"] = "sk-"

"""## Import libraries

## Model building

### Create a model with LangChain

Now we create our model with langchain, using the `RetrievalQA` class:
"""

from langchain import FAISS, PromptTemplate
from langchain_openai import OpenAI, OpenAIEmbeddings
from langchain.document_loaders import PyPDFLoader
from langchain.chains import RetrievalQA
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Prepare vector store (FAISS) with IPPC report
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100, add_start_index=True)
loader = PyPDFLoader("https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.600-1.pdf")
db = FAISS.from_documents(loader.load_and_split(text_splitter), OpenAIEmbeddings())

# Prepare QA chain
PROMPT_TEMPLATE = """You are the Artificial Intelligence Risk Management Assistant, a helpful AI assistant prototyped by JSOC Analytics.
Your task is to answer basic questions on GenAI Risk Management Framework.
You will be given a question and relevant excerpts from the NIST Trustworthy and Responsible AI
NIST AI 600-1 Framework (2024).
Please provide short and clear answers based on the provided context. Be polite and helpful.

Context:
{context}

Question:
{question}

Your answer:
"""

#llm = OpenAI(model="gpt-3.5-turbo", temperature=0)
llm = OpenAI(model="gpt-3.5-turbo-instruct", temperature=0)
prompt = PromptTemplate(template=PROMPT_TEMPLATE, input_variables=["question", "context"])
genAIrisk_qa_chain = RetrievalQA.from_llm(llm=llm, retriever=db.as_retriever(), prompt=prompt)

# Test that everything works
genAIrisk_qa_chain.invoke({"query": "What is a suggested action to address GAI risks regarding data privacy and information integrity?"})

"""It’s working! The answer is coherent with what is stated in the report -- FIXME!!:

> Sea level rise is unavoidable for centuries to millennia due to continuing deep ocean warming and ice sheet melt, and sea levels will remain elevated for thousands of years
>
> (_2023 Climate Change Synthesis Report_, page 77)

## Detect vulnerabilities in your model

### Wrap model and dataset with Giskard

Before running the automatic LLM scan, we need to wrap our model into Giskard's `Model` object. We can also optionally create a small dataset of queries to test that the model wrapping worked.
"""

import giskard
import pandas as pd


def model_predict(df: pd.DataFrame):
    """Wraps the LLM call in a simple Python function.

    The function takes a pandas.DataFrame containing the input variables needed
    by your model, and must return a list of the outputs (one for each row).
    """
    return [genAIrisk_qa_chain.invoke({"query": question}) for question in df["question"]]


# Don’t forget to fill the `name` and `description`: they are used by Giskard
# to generate domain-specific tests.
giskard_model = giskard.Model(
    model=model_predict,
    model_type="text_generation",
    name="GenAI Risk Management Question Answering",
    description="This model answers any question about GenAI risk management based on NIST AI 600-1 Trustworthy and Responsible report",
    feature_names=["question"],
)

"""Let’s check that the model is correctly wrapped by running it over a small dataset:"""

# Optional: let’s test that the wrapped model works
#examples = [
#    "According to the IPCC report, what are key risks in the Europe?",
#    "Is sea level rise avoidable? When will it stop?",
#]

examples = [
    "According to the NIST AI 600-1 report, what are key risks unique to GenAI?",
    "Is harmful bias avoidable? How can it be prevented?",
]

giskard_dataset = giskard.Dataset(pd.DataFrame({"question": examples}), target=None)

print(giskard_model.predict(giskard_dataset).prediction)

"""### Scan your model for vulnerabilities with Giskard

We can now run Giskard's `scan` to generate an automatic report about the model vulnerabilities. This will thoroughly test different classes of model vulnerabilities, such as harmfulness, hallucination, prompt injection, etc.

The scan will use a mixture of tests from predefined set of examples, heuristics, and LLM-based generations and evaluations.

Since running the whole scan can take a bit of time, let’s start by limiting the analysis to the hallucination category:
"""

report = giskard.scan(giskard_model, giskard_dataset, only="hallucination")

display(report)

# Save it to a file
report.to_html("hallucinations_NIST_report.html")

"""### Running the whole scan

We will now run the full scan, testing for all issue categories. Note: this can take up to 30 min, depending on the speed of the API.

Note that the scan results are not deterministic. In fact, LLMs may generally give different answers to the same or similar questions. Also, not all tests we perform are deterministic.
"""

full_report = giskard.scan(giskard_model, giskard_dataset)

"""If you are running in a notebook, you can display the scan report directly in the notebook using `display(...)`, otherwise you can export the report to an HTML file. Check the [API Reference](https://docs.giskard.ai/en/stable/reference/scan/report.html#giskard.scanner.report.ScanReport) for more details on the export methods available on the `ScanReport` class."""

display(full_report)

# Save it to a file
full_report.to_html("NIST_scan_report.html")

"""## Generate comprehensive test suites automatically for your model

### Generate test suites from the scan

The objects produced by the scan can be used as fixtures to generate a test suite that integrates all detected vulnerabilities. Test suites allow you to evaluate and validate your model's performance, ensuring that it behaves as expected on a set of predefined test cases, and to identify any regressions or issues that might arise during development or updates.
"""

test_suite = full_report.generate_test_suite(name="Test suite generated by scan")
test_suite.run()

